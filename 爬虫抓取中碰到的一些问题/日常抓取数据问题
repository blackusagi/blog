爬取的时候，出现错误的地方通过写日志的方式记录错误？
scrapy的爬取不是按照顺序的，这是因为异步的原因吗？
如果今天爬取到5万条数据的时候出现异常，比如403,500等情况，下次爬取的时候，怎么自动执行从当前数据以后的爬取工作？
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
operSSlerror错误
